{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterScraping.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BstOxjvPH9Aq",
        "bUGArK2QH476"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHXqNyuU3oif"
      },
      "source": [
        "# **SCRAPING TWITTER POSTS FOR *PlayStation* and *Xbox***\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "To collect informal opinions about PlayStation and Xbox brands we scraped one of the most used social media : Twitter.\r\n",
        "For this purpose, we did not use the Twitter API, since historical tweets (older than one week) can not be retrieved with the free developer account, but need a payed full account.\r\n",
        "\r\n",
        "Therefore, tweets are extracted with standard scraping techniques, using `Selenium` to simulate a browser that scrolls the Twitter web page containing the results of our advanced query, and `Beautiful Soup` to parse the web content.\r\n",
        "\r\n",
        "\r\n",
        "ABOUT THE DATASETS\r\n",
        "For each brand, we extracted data from 2 different temporal windows:\r\n",
        " - from **22 Sep 2017** to **1 Feb 2018**\r\n",
        " - from **22 Sep 2020** to **1 Feb 2021**\r\n",
        "\r\n",
        "The main reason behind is that we want to retrieve almost recent opinions mostly about the most recent products:\r\n",
        "-  **Xbox Series X** and **S**, released in November 10, 2020,\r\n",
        "-  **Xbox One X**, released in  November 7, 2017\r\n",
        "- **PlayStation 5**, released in November 12, 2020\r\n",
        "\r\n",
        "and also previous consoles as Xbox One, PlayStation 4, ...\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BstOxjvPH9Aq"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlYRj_GA-IS4"
      },
      "source": [
        "# install chromium, its driver, and selenium\r\n",
        "!apt-get update\r\n",
        "!apt install chromium-chromedriver\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\r\n",
        "!pip install selenium\r\n",
        "# set options to be headless, ..\r\n",
        "from selenium import webdriver\r\n",
        "options = webdriver.ChromeOptions()\r\n",
        "options.add_argument('--headless')\r\n",
        "options.add_argument('--no-sandbox')\r\n",
        "options.add_argument('--disable-dev-shm-usage')\r\n",
        "# open it, go to a website, and get results\r\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\r\n",
        "wd.get(\"https://www.website.com\")\r\n",
        "print(wd.page_source)  # results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odPZVTmBimpa"
      },
      "source": [
        "import time\r\n",
        "import re\r\n",
        "import csv\r\n",
        "from selenium import webdriver\r\n",
        "from selenium.webdriver.common.keys import Keys\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from selenium.webdriver.support import expected_conditions as EC\r\n",
        "from selenium.webdriver.support.ui import WebDriverWait"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUGArK2QH476"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXgE2UDET-GA"
      },
      "source": [
        "This function creates a csv file to store scraped data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk9Bo405PuVt"
      },
      "source": [
        "def create_file(filename):\r\n",
        "  with open(filename, 'w') as f:\r\n",
        "    writer = csv.writer(f)\r\n",
        "    writer.writerow(['username', 'datetime', 'tweet_text', 'retweeted_text', \r\n",
        "                     'likes', 'retweets', 'comments'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTYJX3drDdjm"
      },
      "source": [
        "This function is used to scrape tweet information using the advanced query. Also, it avoids to store duplicates of tweets due to the scrolling of the page, taking track of the last stored tweet.\r\n",
        "\r\n",
        "Parameters:\r\n",
        "- `ANDterms` : they are forced to be in each tweet;\r\n",
        "- `ORterms` : at least one between these term should be contained in the resulting tweets;\r\n",
        "- `NOTterms` : none of these terms should be contained;\r\n",
        "- `hashtags` : terms that are forced to be contained in the hashtag format;\r\n",
        "- `lang` : language to filter on;\r\n",
        "- `date_until` : end of the selected temporal window;\r\n",
        "- `date_since` : start of the selected temporal window;\r\n",
        "- `filename` : file in which we have to store the data.\r\n",
        "\r\n",
        "These parameters, expecially `NOTterms` were needed to filter relevant information as much as possible, to overcome the \"*dirty nature*\" of Twitter (full of adv and irrelevant info)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li5yAiPg6K62"
      },
      "source": [
        "def scrape_tweets(ANDterms, ORterms, NOTterms, hashtags, lang, date_until, date_since, filename):\r\n",
        "\r\n",
        "  ## SET URL AND BROWSER WITH SELENIUM\r\n",
        "  browser = webdriver.Chrome('chromedriver',options=options)\r\n",
        "\r\n",
        "  ## CONSTRUCTING THE URL USING QUERY TERMS\r\n",
        "  base_url = u'https://twitter.com/search?q='\r\n",
        "  for t in ANDterms:\r\n",
        "    base_url += t + '%20'\r\n",
        "  if len(ORterms) > 0:\r\n",
        "    base_url += '('\r\n",
        "  for t in ORterms:\r\n",
        "    base_url += t\r\n",
        "    if t == ORterms[-1]:\r\n",
        "      base_url += ')%20'    \r\n",
        "    else:\r\n",
        "      base_url += '%20OR%20'\r\n",
        "  for t in NOTterms:\r\n",
        "    base_url += '-'+ t + '%20'\r\n",
        "  if len(hashtags) > 0:\r\n",
        "    base_url += '('\r\n",
        "    for t in hashtags:\r\n",
        "      base_url += '%23' + t\r\n",
        "      if t == hashtags[-1]:\r\n",
        "        base_url += ')%20'\r\n",
        "  base_url += 'lang%3A' + lang + '%20'\r\n",
        "  base_url += u'until%3A' + date_until + '%20' \r\n",
        "  base_url += u'since%3A'+date_since + '&f=live'  #from recent\r\n",
        "  #base_url += u'since%3A'+date_since #from popular\r\n",
        "  print(base_url)        \r\n",
        "\r\n",
        "  browser.get(base_url)\r\n",
        "  time.sleep(2) #wait that the page is loaded\r\n",
        "  body = browser.find_element_by_tag_name('body')\r\n",
        "\r\n",
        "  count = 0\r\n",
        "\r\n",
        "  last_tweet = \"\"\r\n",
        "  started = True #helps to remember if we already started to scrape tweets (False if we are still checking for overlaps)\r\n",
        "\r\n",
        "  try:\r\n",
        "    while True:   #scroll pages until KeyBoard interruption\r\n",
        "      print(\"\\n\\n . . . . . . SCROLLING PAGE . . . . . .\")\r\n",
        "      body.send_keys(Keys.PAGE_DOWN)\r\n",
        "      time.sleep(0.2) #wait that the content is loaded\r\n",
        "\r\n",
        "      ## GET HTML CONTENT\r\n",
        "      html_source = browser.page_source\r\n",
        "      soup = BeautifulSoup(html_source)\r\n",
        "\r\n",
        "      ## GET TWEETS\r\n",
        "      for d in soup.findAll('article', attrs={'class':'css-1dbjc4n r-1loqt21 r-18u37iz r-1ny4l3l r-1udh08x r-1yt7n81 r-ry3cjt r-o7ynqc r-6416eg'}): \r\n",
        "\r\n",
        "        #Get tweet text and hashtags\r\n",
        "        text_container = d.find('div', attrs={'class':'css-901oao r-18jsvk2 r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0'})\r\n",
        "        text_children = text_container.findChildren('span')\r\n",
        "        text = \"\"\r\n",
        "        for text_tag in text_children:\r\n",
        "          text += text_tag.text\r\n",
        "        text = re.sub('\\s+',' ',text)  #removing multiple spaces and new lines\r\n",
        "\r\n",
        "        if (last_tweet == \"\") or (last_tweet != text and started == True):\r\n",
        "\r\n",
        "          ## DON'T CONSIDER TWEETS WITH http links (just ads, irrelevant)\r\n",
        "          links = d.find('span', attrs={'class':'css-901oao css-16my406 r-poiln3 r-hiw28u r-bcqeeo r-qvutc0'})\r\n",
        "          if links is None:\r\n",
        "\r\n",
        "            count += 1\r\n",
        "\r\n",
        "            #Get username\r\n",
        "            user_tag = d.find('span', attrs={'class':'css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0'})\r\n",
        "            user = user_tag.text\r\n",
        "\r\n",
        "            #Get date time\r\n",
        "            date_tag = d.find('time')\r\n",
        "            date = date_tag['datetime']\r\n",
        "\r\n",
        "            #Get number of likes\r\n",
        "            likes_tag = d.find('div', attrs={'data-testid':'like'})\r\n",
        "            n_likes = (likes_tag['aria-label']).split(\" \")[0]\r\n",
        "\r\n",
        "            #Get number of retweets\r\n",
        "            retweets_tag = d.find('div', attrs={'data-testid':'retweet'})\r\n",
        "            n_retweets = (retweets_tag['aria-label']).split(\" \")[0]\r\n",
        "\r\n",
        "            #Get number of comments\r\n",
        "            comments_tag = d.find('div', attrs={'data-testid':'reply'})\r\n",
        "            n_comments = (comments_tag['aria-label']).split(\" \")[0]\r\n",
        "\r\n",
        "            #Get retwitted text\r\n",
        "            retweetedtext_container = d.find('div', attrs={'class':'css-1dbjc4n r-1bs4hfb r-1867qdf r-rs99b7 r-1loqt21 r-dap0kf r-1ny4l3l r-1udh08x r-o7ynqc r-6416eg'})\r\n",
        "            retweeted_text = \"\"\r\n",
        "            if retweetedtext_container is not None:\r\n",
        "              retweetedtext_children = retweetedtext_container.findChildren('span')\r\n",
        "              for retweetedtext_tag in retweetedtext_children:\r\n",
        "                retweeted_text += retweetedtext_tag.text\r\n",
        "              retweeted_text = re.sub('\\s+',' ',retweeted_text)  #removing multiple spaces and new lines\r\n",
        "      \r\n",
        "            print(\"\\n - (\", count, \") \", user, \"[\", date, \"] >> \", text )\r\n",
        "            if retweeted_text != \"\":\r\n",
        "              print(\"[ RETWEETS >> \", retweeted_text, \" ]\")\r\n",
        "            print(n_likes, \" likes, \", n_retweets, \" retweets, \", n_comments, \" comments\")\r\n",
        "\r\n",
        "            ## APPEND TWEET TO CSV\r\n",
        "            with open(filename, 'a') as f:\r\n",
        "              writer = csv.writer(f)\r\n",
        "              writer.writerow([user, date, text, retweeted_text, n_likes, n_retweets, n_comments])\r\n",
        "\r\n",
        "        else:\r\n",
        "          if last_tweet == text and started == False :\r\n",
        "            started = True #from next tweet element I have to start scraping (no more overlaps)\r\n",
        "    \r\n",
        "      last_tweet = text #update last tweet with the last text of the scrolled page\r\n",
        "      started = False\r\n",
        "\r\n",
        "  except KeyboardInterrupt:     #interrupt the script when you see no new tweet for many ... SCROLLING PAGE ...\r\n",
        "    print('*********** SCRAPING STOPPED! ***********')\r\n",
        "    print(\"\\n\\nScraped  \", count, \" tweets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGLaKVZwF_mr"
      },
      "source": [
        "This method is used for **post-filter** tweets.\r\n",
        "Again, we noticed that the datasets still contained irrelevant information (such as \"Merry Christmas #xbox #xboxseries #playstation\"), so using our intuitions we inspected them, selecting tweets containing certain substrings, and took a decision about what had to be dropped and what was actually relevant.\r\n",
        "\r\n",
        "The parameter `method` has value:\r\n",
        "- '*view*' to just inspect tweets containing the substring (`word`)\r\n",
        "- '*drop*' to drop all tweets containing the given substring;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ItvfEFTZ6t"
      },
      "source": [
        "def view_or_drop_by_word(df,word, method):\r\n",
        "  for index in df.index[df['tweet_text'].str.contains(word)]:\r\n",
        "    if method == 'view':\r\n",
        "      print(index, \" >> \", df.loc[index]['tweet_text'])\r\n",
        "      \r\n",
        "    if method == 'drop':\r\n",
        "      df = df.drop([index])\r\n",
        "  if method == 'drop':\r\n",
        "    print(\"Data after dropping tweets containing word\", word, \": \", df['tweet_text'].count())\r\n",
        "  return df\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6MOZvsaKn_D"
      },
      "source": [
        "## Scraping *PlayStation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOsyNHPKqVLs"
      },
      "source": [
        "### **2020-2021**\r\n",
        "\r\n",
        "In this section we extract data from 22-09-2020 to 01-02-2021, to capture opinions regarding **PlayStation** and ps4 and ps5 products. This window considers the period before and after that ps5 was out (November 2020)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLyg4ocFDJx"
      },
      "source": [
        "## SETTING THE ADVANCED QUERY\r\n",
        "ANDterms = ['playstation'] #all of these terms are mandatory\r\n",
        "ORterms = ['ps3','ps4', 'ps5', 'playstation3', 'playstation4', 'playstation5'] #at least one of these terms should be included\r\n",
        "NOTterms = ['follow', 'followme', 'check', 'giveaway', 'giveaways', 'page', 'twitch',\r\n",
        "            'live', 'livestream', 'PS4share', 'PS5share', 'eBay', 'screenshot', 'youtube',\r\n",
        "            'challenge', 'dm', 'shots', 'video', 'stream', 'Ad'] #none of these terms (probably add spam and irrelevant info)\r\n",
        "hashtags = ['PlayStation'] #all these hashtags\r\n",
        "lang = 'en'\r\n",
        "date_until = '2021-02-02'\r\n",
        "date_since = '2020-09-22'\r\n",
        "\r\n",
        "\r\n",
        "## PREPARING THE FILE\r\n",
        "filename = 'playstation_' + date_since + '_' + date_until + '_tweets.txt'\r\n",
        "create_file(filename)\r\n",
        "\r\n",
        "scrape_tweets(ANDterms, ORterms, NOTterms, hashtags, lang, date_until, date_since, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vYiV6-4hmD",
        "outputId": "df592959-32df-4c3b-ed3e-3a6a4b5414f3"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(r'playstation_2020-09-22_2021-02-02_tweet.csv')\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21777, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLgWUSJ4LXtM"
      },
      "source": [
        "We dropped duplicates due to retweeted texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueeQRu2s07bq",
        "outputId": "3423c139-bf46-4b22-df3f-96f38564a53b"
      },
      "source": [
        "print(\"Data before dropping duplicate texts:\\n \", df.count())\r\n",
        "df.drop_duplicates(subset =\"tweet_text\",  keep = False, inplace = True)\r\n",
        "print(\"Data after dropping duplicate texts:\\n \", df.count())\r\n",
        "#NOTE: some usernames are empty because they contain strange characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data before dropping duplicate texts:  username          25804\n",
            "datetime          25854\n",
            "tweet_text        25854\n",
            "retweeted_text    1055 \n",
            "likes             25854\n",
            "retweets          25854\n",
            "comments          25854\n",
            "dtype: int64\n",
            "Data after dropping duplicate texts:  username          21728\n",
            "datetime          21777\n",
            "tweet_text        21777\n",
            "retweeted_text    1010 \n",
            "likes             21777\n",
            "retweets          21777\n",
            "comments          21777\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpjIaQV4PnIs"
      },
      "source": [
        "We **post-filtered** tweets to remove irrelevant information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "042Kwbx0Px3j"
      },
      "source": [
        "view_or_drop_by_word(df, 'Merry Christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Merry Christmas', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Happy new year', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Happy new year', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Click', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Click', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'followers', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'followers', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'contact me', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'contact me', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'watch me', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'watch me', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'twitchstreamers', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'twitchstreamers', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Gaming News Drop', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Gaming News Drop', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'twitchstreams', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'twitchstreams', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'News Drop', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'News Drop', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Join', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Join', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'giving away', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'giving away', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'link in bio', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'link in bio', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'join me', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'join me', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'FREE DOWNLOAD', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'FREE DOWNLOAD', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'FREE', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'FREE', 'drop')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXd3JbuUL4zk"
      },
      "source": [
        "The dataset was chunked to avoid crashes during the preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tRNbZ--BHnU",
        "outputId": "5cdd1fe8-87f6-4b88-d1f6-1fe9ce89bb40"
      },
      "source": [
        "GROUP_SIZE = 4000\r\n",
        "\r\n",
        "def chunk(seq, size):\r\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\r\n",
        "\r\n",
        "i = 0\r\n",
        "for df_chunk in chunk(df, GROUP_SIZE):\r\n",
        "  i += 1\r\n",
        "  print(df_chunk['tweet_text'].count())\r\n",
        "  file_name = 'playstation_2020-09-22_2021-02-02_tweets' + str(i) + '.csv'\r\n",
        "  print(file_name, \"saved.\")\r\n",
        "  df_chunk.to_csv(file_name, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n",
            "playstation_2020-09-22_2021-02-02_tweets1.csv saved.\n",
            "4000\n",
            "playstation_2020-09-22_2021-02-02_tweets2.csv saved.\n",
            "4000\n",
            "playstation_2020-09-22_2021-02-02_tweets3.csv saved.\n",
            "4000\n",
            "playstation_2020-09-22_2021-02-02_tweets4.csv saved.\n",
            "4000\n",
            "playstation_2020-09-22_2021-02-02_tweets5.csv saved.\n",
            "1474\n",
            "playstation_2020-09-22_2021-02-02_tweets6.csv saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKAovthI5fUg"
      },
      "source": [
        "### **2017-2018**\r\n",
        "\r\n",
        "We extract data from 22-09-2017 to 01-02-2018, to capture opinions regarding **PlayStation** and ps3 and ps4 products. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNP3bZhx5ejO"
      },
      "source": [
        "## SETTING THE ADVANCED QUERY\r\n",
        "ANDterms = ['playstation'] #all of these terms are mandatory\r\n",
        "ORterms = ['ps3','ps4', 'ps2', 'playstation2' 'playstation3', 'playstation4', 'psp'] #at least one of these terms should be included\r\n",
        "NOTterms = ['follow', 'followme', 'check', 'giveaway', 'giveaways', 'page', 'twitch',\r\n",
        "            'live', 'livestream', 'PS4share', 'PS5share', 'eBay', 'screenshot', 'youtube',\r\n",
        "            'challenge', 'dm', 'shots', 'video', 'stream', 'Ad'] #none of these terms (probably add spam and irrelevant info)\r\n",
        "hashtags = ['PlayStation'] #all these hashtags\r\n",
        "lang = 'en'\r\n",
        "date_until = '2018-02-02'\r\n",
        "date_since = '2017-09-22'\r\n",
        "\r\n",
        "\r\n",
        "## PREPARING THE FILE\r\n",
        "filename = 'playstation_' + date_since + '_' + date_until + '_tweets.txt'\r\n",
        "create_file(filename)\r\n",
        "\r\n",
        "scrape_tweets(ANDterms, ORterms, NOTterms, hashtags, lang, date_until, date_since, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybv69qo4F3sv",
        "outputId": "970ca240-93c0-4129-83b1-6fd7b6baf62f"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(r'playstation_2017-09-22_2018-02-02_tweet.csv')\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4290, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hqGzhlgQfAk",
        "outputId": "68f1ce50-d4b0-4496-b99b-f2554a00d506"
      },
      "source": [
        "print(\"Data before dropping duplicate texts:\\n \", df.count())\r\n",
        "df.drop_duplicates(subset =\"tweet_text\",  keep = False, inplace = True)\r\n",
        "print(\"Data after dropping duplicate texts:\\n \", df.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data before dropping duplicate texts:\n",
            "  username          4730\n",
            "datetime          4741\n",
            "tweet_text        4741\n",
            "retweeted_text    70  \n",
            "likes             4741\n",
            "retweets          4741\n",
            "comments          4741\n",
            "dtype: int64\n",
            "Data after dropping duplicate texts:\n",
            "  username          4279\n",
            "datetime          4290\n",
            "tweet_text        4290\n",
            "retweeted_text    66  \n",
            "likes             4290\n",
            "retweets          4290\n",
            "comments          4290\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmV0Q5YoF0ZI"
      },
      "source": [
        "**Post filtering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-caMoUCGOCe",
        "outputId": "452b56c1-c5cd-4662-ec94-803432d05e41"
      },
      "source": [
        "view_or_drop_by_word(df, 'Merry Christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Merry Christmas', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Happy new year', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Happy new year', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'followers', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'followers', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'contact me', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'contact me', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Join', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Join', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'my channel', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'my channel', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'youtuber', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'youtuber', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'See others view', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'See others view', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'How to solve', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'How to solve', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'New Daily Update', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'New Daily Update', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'NEW', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'NEW', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'How to Solve', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'How to Solve', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, '#YoutubeChannel #Youtuber', 'view')\r\n",
        "df = view_or_drop_by_word(df, '#YoutubeChannel #Youtuber', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'In-Stock Today', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'In-Stock Today', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, '#ArGameShowForMe', 'view')  #just spam \r\n",
        "df = view_or_drop_by_word(df, '#ArGameShowForMe', 'drop')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data after dropping tweets containing word Merry Christmas :  4047\n",
            "Data after dropping tweets containing word Happy new year :  4047\n",
            "Data after dropping tweets containing word followers :  4047\n",
            "Data after dropping tweets containing word contact me :  4047\n",
            "Data after dropping tweets containing word Join :  4047\n",
            "Data after dropping tweets containing word my channel :  4047\n",
            "Data after dropping tweets containing word youtuber :  4047\n",
            "Data after dropping tweets containing word See others view :  4047\n",
            "Data after dropping tweets containing word How to solve :  4047\n",
            "Data after dropping tweets containing word New Daily Update :  4047\n",
            "Data after dropping tweets containing word NEW :  4047\n",
            "Data after dropping tweets containing word How to Solve :  4047\n",
            "Data after dropping tweets containing word #YoutubeChannel #Youtuber :  4047\n",
            "Data after dropping tweets containing word In-Stock Today :  4047\n",
            "3369  >>  #ArGameShowForMe #PlayStation ps4 + fifa18 today\n",
            "3375  >>  #ArGameShowForMe #PlayStation #ArGameShowForMe #Overwatch #ArGameShowForMe#PlayStation #ArGameShowForMe#Overwatch Quiero una PlayStation4\n",
            "3378  >>  #ArGameShowForMe #PlayStation argameshow ps4 argameshow ps4 argameshow ps4\n",
            "3379  >>  #ArGameShowForMe #PlayStation argameshow ps4\n",
            "3380  >>  #ArGameShowForMe #PlayStation ps4 argameshow ps4 argameshow ps4 argameshow ps4 argameshow ps4 argameshow ps4 argameshow ps4 argameshow\n",
            "3381  >>  #ArGameShowForMe #PlayStation ps4 argameshow ps4 argameshow ps4 argameshow\n",
            "3382  >>  #ArGameShowForMe #PlayStation ps4 argameshow ps4 argameshow\n",
            "3383  >>  #ArGameShowForMe #PlayStation ps4 argameshow\n",
            "3385  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4\n",
            "3386  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4\n",
            "3387  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4 ps4\n",
            "3388  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4 ps4 ps4 ps4\n",
            "3389  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4 ps4 ps4\n",
            "3390  >>  #ArGameShowForMe #PlayStation ps4 ps4 ps4\n",
            "3391  >>  #ArGameShowForMe #PlayStation ps4 ps4\n",
            "3392  >>  #ArGameShowForMe #PlayStation want that ps4\n",
            "3414  >>  #ArGameShowForMe #PlayStation ps4 for me\n",
            "3430  >>  #ArGameShowForMe #PlayStation concurso gameshowAR PS4+FIFA18\n",
            "3437  >>  #ArGameShowForMe #PlayStation GameShow Argentina , ps4 + fifa18\n",
            "Data after dropping tweets containing word #ArGameShowForMe :  4028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NVO3NFqLK9O",
        "outputId": "efa3e1da-33ab-48e9-85c6-fbd7329289c5"
      },
      "source": [
        "#save the post-filtered dataset\r\n",
        "df.to_csv('playstation_2017-09-22_2018-02-02_tweets.csv', encoding='utf-8', index=False)\r\n",
        "#check that everything is done correctly\r\n",
        "df = pd.read_csv(r'playstation_2017-09-22_2018-02-02_tweets.csv')\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4028, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyA0QavYSICi"
      },
      "source": [
        "## Scraping **Xbox**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYzQVaaCTcMw"
      },
      "source": [
        "### **2020-2021**\r\n",
        "\r\n",
        "In this section we extract data from 22-09-2020 to 01-02-2021, to capture opinions regarding **Xbox**, in particular for Xbox series s and x products. This window considers the period before and after that series was out (November 2020)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIjEte_VTiCa"
      },
      "source": [
        "## SETTING THE ADVANCED QUERY\r\n",
        "ANDterms = ['xbox'] #all of these terms are mandatory\r\n",
        "ORterms = ['xbox360', 'xbox1', 'xboxone', 'xboxonex', 'xboxones', \r\n",
        "           'xboxseries', 'xboxseriesx', 'xboxseriess'] #at least one of these terms should be included\r\n",
        "NOTterms = ['follow', 'followme', 'check', 'giveaway', 'giveaways', 'page', 'twitch',\r\n",
        "            'live', 'livestream', 'eBay', 'screenshot', 'youtube',\r\n",
        "            'challenge', 'dm', 'shots', 'video', 'stream', 'Ad'] #none of these terms (probably add spam and irrelevant info)\r\n",
        "hashtags = ['xbox'] #all these hashtags\r\n",
        "lang = 'en'\r\n",
        "date_until = '2021-02-02'\r\n",
        "date_since = '2020-09-22'\r\n",
        "\r\n",
        "\r\n",
        "## PREPARING THE FILE\r\n",
        "filename = 'xbox_' + date_since + '_' + date_until + '_tweets.txt'\r\n",
        "create_file(filename)\r\n",
        "\r\n",
        "scrape_tweets(ANDterms, ORterms, NOTterms, hashtags, lang, date_until, date_since, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjN_14OQPSAV",
        "outputId": "f653b4af-64b6-4cc1-be68-5f010e56ccf4"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(r'xbox_2020-09-22_2021-02-02_tweets.txt')\r\n",
        "print(df.tail())\r\n",
        "\r\n",
        "print(\"Data before dropping duplicate texts:\\n \", df.count())\r\n",
        "df.drop_duplicates(subset =\"tweet_text\",  keep = False, inplace = True)\r\n",
        "print(\"Data after dropping duplicate texts:\\n \", df.count())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               username                  datetime  ... retweets comments\n",
            "36864         Boldsmack  2020-09-22T00:14:26.000Z  ...        2        0\n",
            "36865     bryan tabares  2020-09-22T00:11:46.000Z  ...        0        0\n",
            "36866  Chinmaya Nathany  2020-09-22T00:03:14.000Z  ...        0        1\n",
            "36867            bjcro0  2020-09-22T00:02:54.000Z  ...        0        1\n",
            "36868       playgames99  2020-09-22T00:02:00.000Z  ...        0        0\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "Data before dropping duplicate texts:\n",
            "  username          36784\n",
            "datetime          36869\n",
            "tweet_text        36869\n",
            "retweeted_text     2126\n",
            "likes             36869\n",
            "retweets          36869\n",
            "comments          36869\n",
            "dtype: int64\n",
            "Data after dropping duplicate texts:\n",
            "  username          30083\n",
            "datetime          30156\n",
            "tweet_text        30156\n",
            "retweeted_text     1916\n",
            "likes             30156\n",
            "retweets          30156\n",
            "comments          30156\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIMG3xGaSo2d"
      },
      "source": [
        "**Post filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSzV39VZPnQp"
      },
      "source": [
        "view_or_drop_by_word(df, 'followers', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'followers', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'happy new', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'happy new', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'merry christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'merry christmas', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Click', 'view') #just spam and ads\r\n",
        "df = view_or_drop_by_word(df, 'Click', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Happy new year', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Happy new year', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Merry Christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Merry Christmas', 'drop')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "V5g-PGYiPouZ",
        "outputId": "72588257-dc1c-45c4-e572-dcec0e80fdba"
      },
      "source": [
        "#Save filtered dataset in the csv\r\n",
        "df.to_csv('xbox_2020-09-22_2021-02-02_tweets.csv', encoding='utf-8', index=False)\r\n",
        "df = pd.read_csv(r'xbox_2020-09-22_2021-02-02_tweets.csv')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>datetime</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>retweeted_text</th>\n",
              "      <th>likes</th>\n",
              "      <th>retweets</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Desbee Jr.</td>\n",
              "      <td>2021-02-01T23:55:50.000Z</td>\n",
              "      <td>My first #darkmatterultra clip it was a good 5...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xboxnintendo222</td>\n",
              "      <td>2021-02-01T23:44:40.000Z</td>\n",
              "      <td>No photo mode! No cutscenes! In game bro! I Xb...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Germanicus</td>\n",
              "      <td>2021-02-01T23:37:14.000Z</td>\n",
              "      <td>\"Done\" #PS5 #XboxSeriesX #XboxSeries #Sony #Xbox</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>( FoAm ) XxDaddyxX</td>\n",
              "      <td>2021-02-01T23:27:01.000Z</td>\n",
              "      <td>* XBOX * Thursday Night FoAm 7:30 PM EST Battl...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>David Ross</td>\n",
              "      <td>2021-02-01T23:26:02.000Z</td>\n",
              "      <td>Man those days were a nightmare. The WiFi adap...</td>\n",
              "      <td>NeoNeo@NeoGameSparkFeb 1Ah man, I remember tho...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             username                  datetime  ... retweets comments\n",
              "0          Desbee Jr.  2021-02-01T23:55:50.000Z  ...        1        0\n",
              "1  xboxnintendo222     2021-02-01T23:44:40.000Z  ...        1        1\n",
              "2          Germanicus  2021-02-01T23:37:14.000Z  ...        2        0\n",
              "3  ( FoAm ) XxDaddyxX  2021-02-01T23:27:01.000Z  ...        0        0\n",
              "4          David Ross  2021-02-01T23:26:02.000Z  ...        0        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--WT2jTJS3m0"
      },
      "source": [
        "Chunking data to make preprocessing easier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa19OWGTh0Bp",
        "outputId": "8afce252-89cd-4f93-eadc-729d0b718f7f"
      },
      "source": [
        "GROUP_SIZE = 4000\r\n",
        "\r\n",
        "def chunk(seq, size):\r\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\r\n",
        "\r\n",
        "i = 0\r\n",
        "for df_chunk in chunk(df, GROUP_SIZE):\r\n",
        "  i += 1\r\n",
        "  file_name = 'xbox_2020-09-22_2021-02-02_tweets' + str(i) + '.csv'\r\n",
        "  print(file_name, \"saved.\")\r\n",
        "  df_chunk.to_csv(file_name, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xbox_2020-09-22_2021-02-02_tweets1.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets2.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets3.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets4.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets5.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets6.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets7.csv saved.\n",
            "xbox_2020-09-22_2021-02-02_tweets8.csv saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmgAnAKxn3yx"
      },
      "source": [
        "### **2017-2018**\r\n",
        "\r\n",
        "In this section we extract data from 22-09-2020 to 01-02-2021, to capture opinions regarding older **Xbox** consoles, in particular for Xbox 360 and xbox one products. This window considers the period before and after that Xbox One X was out (November 2017)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGz46gf8nxzs"
      },
      "source": [
        "## SETTING THE ADVANCED QUERY\r\n",
        "ANDterms = ['xbox'] #all of these terms are mandatory\r\n",
        "ORterms = ['xbox360', 'xbox1', 'xboxone', 'xboxonex', 'xboxones'] #at least one of these terms should be included\r\n",
        "NOTterms = ['follow', 'followme', 'check', 'giveaway', 'giveaways', 'page', 'twitch',\r\n",
        "            'live', 'livestream', 'eBay', 'screenshot', 'youtube',\r\n",
        "            'challenge', 'dm', 'shots', 'video', 'stream', 'Ad', 'followers', 'follower'] #none of these terms (probably add spam and irrelevant info)\r\n",
        "hashtags = ['xbox'] #all these hashtags\r\n",
        "lang = 'en'\r\n",
        "date_until = '2018-02-02'\r\n",
        "date_since = '2017-09-22'\r\n",
        "\r\n",
        "\r\n",
        "## PREPARING THE FILE\r\n",
        "filename = 'xbox_' + date_since + '_' + date_until + '_tweets.txt'\r\n",
        "create_file(filename)\r\n",
        "\r\n",
        "scrape_tweets(ANDterms, ORterms, NOTterms, hashtags, lang, date_until, date_since, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gyly51tLZJJ",
        "outputId": "df0057ec-522e-4ef6-d739-41c5ad844e3f"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(r'xbox_2017-09-22_2018-02-02_tweets.txt')\r\n",
        "print(df.tail())\r\n",
        "\r\n",
        "print(\"Data before dropping duplicate texts:\\n \", df.count())\r\n",
        "df.drop_duplicates(subset =\"tweet_text\",  keep = False, inplace = True)\r\n",
        "print(\"Data after dropping duplicate texts:\\n \", df.count())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               username                  datetime  ... retweets comments\n",
            "5500     Hangout Gaming  2017-09-22T13:11:27.000Z  ...        0        0\n",
            "5501       The VR Realm  2017-09-22T11:54:44.000Z  ...        0        0\n",
            "5502                Mac  2017-09-22T11:49:09.000Z  ...        0        0\n",
            "5503              Jade   2017-09-22T11:08:34.000Z  ...        0        2\n",
            "5504  Alessandro Biollo  2017-09-22T10:18:00.000Z  ...        0        0\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "Data before dropping duplicate texts:\n",
            "  username          5499\n",
            "datetime          5505\n",
            "tweet_text        5505\n",
            "retweeted_text      93\n",
            "likes             5505\n",
            "retweets          5505\n",
            "comments          5505\n",
            "dtype: int64\n",
            "Data after dropping duplicate texts:\n",
            "  username          4683\n",
            "datetime          4689\n",
            "tweet_text        4689\n",
            "retweeted_text      87\n",
            "likes             4689\n",
            "retweets          4689\n",
            "comments          4689\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZgYi-sTjWi"
      },
      "source": [
        "**Post filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXQfGX7FLmsV"
      },
      "source": [
        "df = view_or_drop_by_word(df, 'followers', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'happy new', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'happy new', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'merry christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'merry christmas', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Click', 'view') #just spam and ads\r\n",
        "df = view_or_drop_by_word(df, 'Click', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Happy new year', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Happy new year', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Merry Christmas', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Merry Christmas', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'sexy', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'sexy', 'drop')\r\n",
        "\r\n",
        "view_or_drop_by_word(df, 'Merry', 'view')\r\n",
        "df = view_or_drop_by_word(df, 'Merry', 'drop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ArvLdfLvAt"
      },
      "source": [
        "#Store the post filtered data in csv\r\n",
        "df.to_csv('xbox_2017-09-22_2018-02-02_tweets.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}